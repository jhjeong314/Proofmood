{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arith5 is an extension of Arith4.  \n",
    "\n",
    "Added components are:\n",
    "* unary prefix -\n",
    "* unary postfix !\n",
    "* exponentiation a^b\n",
    "\n",
    "-a^b is -(a^b), not (-a)^b  \n",
    "a^b! = a^(b!), not (a^b)!  \n",
    "-a! is -(a!), not (-a)!  \n",
    "a^b^c is a^(b^c), not (a^b)^c (right associative)\n",
    "\n",
    "```perl\n",
    "<expr> ::= (<term> | <nterm>) { (\"+\" | \"-\") <term> }\n",
    "<nterm> ::= \"-\" { \"-\" } <term>\n",
    "<term> ::= <factor> { (\"*\" | \"/\") <factor> }\n",
    "<factor> ::= { <factor_exp> \"^\" } <factor_exp>\n",
    "<factor_exp> ::= <factor_post> { (\"!\" | \"'\") }\n",
    "<factor_post> ::= \"(\" <expr> \")\" | <atom>\n",
    "<atom> ::= <identifier> | <numeral>\n",
    "<identifier> ::= <letter> { <letter> | <digit> }\n",
    "<letter> ::= [a-z] \n",
    "<numeral> ::= <positive_digit> { <digit> }\n",
    "<digit> ::= [0-9]\n",
    "<positive_digit> :: = [1-9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "  def __init__(self, value):\n",
    "    self.value = value\n",
    "    # input value is guaranteed to be a valid token\n",
    "    if value in (\"+\", \"-\"):\n",
    "      self.token_type = 'op_bin_1' # precedence 1\n",
    "    elif value in (\"*\", \"/\"):\n",
    "      self.token_type = 'op_bin_2' # precedence 2\n",
    "    elif value == \"(\":\n",
    "      self.token_type = 'lparen'\n",
    "    elif value == \")\":\n",
    "      self.token_type = 'rparen'\n",
    "    elif value in (\"!\", \"'\"):\n",
    "      self.token_type = 'op_postfix'\n",
    "    elif value == \"^\":\n",
    "      self.token_type = \"op_bin_exp\"\n",
    "    elif value.isdecimal():\n",
    "      self.token_type = 'numeral'\n",
    "    elif value.isalnum() and value[0].isalpha():\n",
    "      self.token_type = 'identifier'\n",
    "    else:\n",
    "      raise ValueError(f\"'{value}' is invalid (Token)\")\n",
    "  \n",
    "  def __str__(self):\n",
    "    return f'{self.value} ({self.token_type})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ (op_bin_1)\n",
      "* (op_bin_2)\n",
      "( (lparen)\n",
      ") (rparen)\n",
      "13 (numeral)\n",
      "abc (identifier) a1 (identifier)\n",
      "! (op_postfix) ' (op_postfix)\n",
      "^ (op_bin_exp)\n"
     ]
    }
   ],
   "source": [
    "print(Token(\"+\"))\n",
    "print(Token(\"*\"))\n",
    "print(Token(\"(\"))\n",
    "print(Token(\")\"))\n",
    "print(Token(\"13\"))\n",
    "print(Token(\"abc\"), Token(\"a1\"))\n",
    "print(Token(\"!\"), Token(\"'\"))\n",
    "print(Token(\"^\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(input_text):\n",
    "  tokens = []\n",
    "  # split the input text into a list of tokens at word boundries and whitespaces\n",
    "  # then remove empty strings and strip off leading and trailing whitespaces\n",
    "  li = [s.strip() for s in re.split(r\"\\b|\\s\", input_text, re.ASCII) \n",
    "                  if s.strip()]\n",
    "  for s in li: # s is a string\n",
    "    if not s.isascii():\n",
    "      raise ValueError(f\"'{s}' is invalid (non-ASCII)\")\n",
    "    if not (set(s).issubset(\"+-*/()!'^\") or      # operator or parenthesis\n",
    "            (s.isdecimal() and s[0]!='0') or  # numeral\n",
    "            (s.isalnum() and s[0].isalpha() and s.islower())):   \n",
    "                                              # identifier\n",
    "      raise ValueError(f\"'{s}' is invalid (non-token)\")\n",
    "    if set(s).issubset(\"+-*/()!'^\") and len(s) > 1:\n",
    "      # split string of consecutive operators into individual characters\n",
    "      for c in s: # c is an operator character\n",
    "        tokens.append(Token(c))\n",
    "    else:\n",
    "      tokens.append(Token(s))\n",
    "  \n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTokenizer(input_text):\n",
    "  try:\n",
    "    tokens = tokenizer(input_text)\n",
    "  except ValueError as e:\n",
    "    print(f\"Tokenizer: {e}\")\n",
    "  else:\n",
    "    for t in tokens:\n",
    "      print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (identifier)\n",
      "/ (op_bin_2)\n",
      "b (identifier)\n",
      "+ (op_bin_1)\n",
      "b1 (identifier)\n",
      "^ (op_bin_exp)\n",
      "b (identifier)\n",
      "' (op_postfix)\n",
      "* (op_bin_2)\n",
      "c1 (identifier)\n",
      "! (op_postfix)\n",
      "* (op_bin_2)\n",
      "( (lparen)\n",
      "hello (identifier)\n",
      "- (op_bin_1)\n",
      "cc1 (identifier)\n",
      ") (rparen)\n",
      "+ (op_bin_1)\n",
      "a23 (identifier)\n"
     ]
    }
   ],
   "source": [
    "testTokenizer(\"a/b + b1^b'*c1!* (hello - cc1)+a23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: 'Hello' is invalid (non-token)\n",
      "Tokenizer: '023' is invalid (non-token)\n"
     ]
    }
   ],
   "source": [
    "testTokenizer(\"first + second* +Hello + 23+2\")\n",
    "testTokenizer(\"first + second*-hello + 023+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "  def __init__(self, token, children=None):\n",
    "    self.token = token # the node is labeled with a Token object\n",
    "    self.children = children if children else [] # list of Node objects\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.build_polish_notation()\n",
    "\n",
    "  def build_polish_notation(self, opt=False):\n",
    "    ret_str = (f\"{self.token.value}({self.token.token_type})\" if opt \n",
    "      else f\"{self.token.value}\")\n",
    "    if self.children:\n",
    "      ret_str += ' '\n",
    "    ret_str += ' '.join(child.build_polish_notation(opt) \n",
    "                        for child in self.children)\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "  def __init__(self, tokens):\n",
    "    self.tokens = tokens\n",
    "    self.current_token = None\n",
    "    self.index = -1\n",
    "    self.advance()  # set self.current_token to \n",
    "                    # the first(i.e. self.index=0) element of tokens\n",
    "\n",
    "  def advance(self): # increment self.index and set self.current_token\n",
    "    self.index += 1\n",
    "    if self.index < len(self.tokens):\n",
    "      self.current_token = self.tokens[self.index]\n",
    "    else:\n",
    "      self.current_token = None\n",
    "\n",
    "  def check_token_type(self, token_types):\n",
    "    # token_types can be a string or a tuple of strings\n",
    "    token = self.current_token\n",
    "    if token is None:\n",
    "      return False\n",
    "    elif(type(token_types) is not tuple): # must be a string in this case\n",
    "        return token.token_type == token_types\n",
    "    elif len(token_types) == 1:\n",
    "      return token.token_type == token_types[0]\n",
    "    elif token.token_type in token_types:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "  def parse(self):\n",
    "    return self.expr() # expr() corresponds to the starting symbol <expr>\n",
    "\n",
    "  def expr(self):\n",
    "    if(self.current_token is not None and  \n",
    "      self.current_token.value == '-'): # unary minus\n",
    "      node = self.nterm()\n",
    "    else: # not a negative term\n",
    "      node = self.term()\n",
    "\n",
    "    while self.check_token_type('op_bin_1'): # '+' or '-'\n",
    "      # If we are at '+' in \"a + b * c - ...\" then the next token is '-'\n",
    "      # because we will consume tokens by self.advance() and self.term().\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      right_term = self.term()\n",
    "      node = Node(token, [node, right_term]) # left associative\n",
    "\n",
    "    return node\n",
    "  \n",
    "  def nterm(self):\n",
    "    token = self.current_token \n",
    "    # For the first visit only, token.value == '-' is  guaranteed\n",
    "    #   because we have checked it in self.expr().\n",
    "    # But for subsequent recursive calls it can be otherwise.\n",
    "    if(token is None or token.value != '-'):\n",
    "      node = self.term()\n",
    "    else:\n",
    "      token.token_type = 'op_unary_prefix'\n",
    "      self.advance()\n",
    "      unary_node = self.nterm() # recursive call\n",
    "      node = Node(token, [unary_node])\n",
    "\n",
    "    return node\n",
    "  \n",
    "  def term(self):\n",
    "    node = self.factor()\n",
    "\n",
    "    while self.check_token_type('op_bin_2'): # '*' or '/'\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      right_factor = self.factor()\n",
    "      node = Node(token, [node, right_factor])\n",
    "\n",
    "    return node\n",
    "\n",
    "  def factor(self):\n",
    "    node = self.factor_exp()\n",
    "\n",
    "    if self.check_token_type('op_bin_exp'): # '^'\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      right_factor = self.factor() # recursive call for right associativity\n",
    "      node = Node(token, [node, right_factor])\n",
    "\n",
    "    return node\n",
    "  \n",
    "  def factor_exp(self):\n",
    "    node = self.factor_postfix()\n",
    "\n",
    "    while self.check_token_type('op_postfix'):\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      node = Node(token, [node])\n",
    "\n",
    "    return node\n",
    "\n",
    "  def factor_postfix(self):\n",
    "    if self.check_token_type('lparen'):\n",
    "      self.advance()\n",
    "      node = self.expr()\n",
    "      if self.check_token_type('rparen'):\n",
    "        self.advance()\n",
    "      else:\n",
    "        raise SyntaxError(\"Expected ')' after expression at {self.index}, in factor_postfix()\")\n",
    "    else:\n",
    "      node = self.atom()\n",
    "\n",
    "    return node\n",
    "\n",
    "  def atom(self):\n",
    "    if self.current_token is not None:\n",
    "      token = self.current_token\n",
    "      if self.check_token_type(('numeral', 'identifier')):\n",
    "        self.advance()\n",
    "        return Node(token)\n",
    "      else:\n",
    "        raise SyntaxError(f\"Expected numeral or identifier at {self.index}, in atom(): {token}\")\n",
    "    else:\n",
    "      raise SyntaxError(\"Unexpected end of input, in atom()\")\n",
    "      \n",
    "def parse_input(input_text):\n",
    "  tokens = tokenizer(input_text)\n",
    "  parser = Parser(tokens)\n",
    "  ast = parser.parse() # ast = Abstract Syntax Tree\n",
    "  if parser.current_token is not None:\n",
    "    raise SyntaxError(f\"Unexpected token {parser.current_token} at {parser.index}, in parse_input(). Expected end of input.\")\n",
    "  return ast\n",
    "\n",
    "def testParser(input_text, showOperType=False):\n",
    "  try:\n",
    "    ast = parse_input(input_text)\n",
    "  except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "  except SyntaxError as e:\n",
    "    print(f\"SyntaxError: {e}\")\n",
    "  else:\n",
    "    print(ast.build_polish_notation(showOperType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - a - * b c\n"
     ]
    }
   ],
   "source": [
    "testParser(\"--a - (-b * c)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ - * a b2 c\n",
      "+ - * a - - b2 c\n",
      "SyntaxError: Expected numeral or identifier at 2, in atom(): - (op_bin_1)\n",
      "+ + first * second + hello - c1 a23\n",
      "-(op_bin_1) a(identifier) -(op_unary_prefix) b(identifier)\n",
      "SyntaxError: Expected numeral or identifier at 2, in atom(): + (op_bin_1)\n",
      "SyntaxError: Unexpected end of input, in atom()\n",
      "SyntaxError: Unexpected token b (identifier) at 1, in parse_input(). Expected end of input.\n"
     ]
    }
   ],
   "source": [
    "testParser(\"-a*b2 + c\")\n",
    "testParser(\"-a*(--b2) + c\")\n",
    "testParser(\"a + -b\")\n",
    "testParser(\"first + second* (hello + (-c1))+a23\")\n",
    "testParser(\"a - (-b)\", showOperType=True)\n",
    "# Some invalid expressions.\n",
    "testParser(\"a + + b\")\n",
    "testParser(\"a + b *\")\n",
    "testParser(\"a b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * + - ! a b b ' ! ! c\n"
     ]
    }
   ],
   "source": [
    "testParser(\"(-a!+b)*b*c!!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^ a ^ b c\n",
      "^ a ^ - - - ! ' b c\n",
      "^ - ^ a ! b c\n"
     ]
    }
   ],
   "source": [
    "testParser(\"a^b^c\") # right associativity\n",
    "testParser(\"(a)^(---b'!)^c\")\n",
    "testParser(\"(-a^b!)^c\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
