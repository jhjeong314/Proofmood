{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arith4 is a simple grammar for arithmetic.\n",
    "We'll build a parser for this grammar.\n",
    "\n",
    "```\n",
    "<expr> ::= <term> { (\"+\" | \"-\") <term> }\n",
    "<term> ::= <factor> { (\"*\" | \"/\") <factor> }\n",
    "<factor> ::= \"(\" <expr> \")\" | <atom>\n",
    "<atom> ::= <identifier> | <numeral>\n",
    "<identifier> ::= <letter> { <letter> | <digit> }\n",
    "<letter> ::= [a-z] \n",
    "<numeral> ::= <positive_digit> { <digit> }\n",
    "<digit> ::= [0-9]\n",
    "<positive_digit> :: = [1-9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "  def __init__(self, value):\n",
    "    self.value = value\n",
    "    # input value is guaranteed to be a valid token\n",
    "    if value in (\"+\", \"-\"):\n",
    "      self.token_type = 'op_type1' # precedence 1\n",
    "    elif value in (\"*\", \"/\"):\n",
    "      self.token_type = 'op_type2' # precedence 2\n",
    "    elif value == \"(\":\n",
    "      self.token_type = 'lparen'\n",
    "    elif value == \")\":\n",
    "      self.token_type = 'rparen'\n",
    "    elif value.isdecimal():\n",
    "      self.token_type = 'numeral'\n",
    "    elif value.isalnum() and value[0].isalpha():\n",
    "      self.token_type = 'identifier'\n",
    "    else:\n",
    "      raise ValueError(f\"'{value}' is invalid token\")\n",
    "  \n",
    "  def __str__(self):\n",
    "    return f'{self.value} ({self.token_type})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ (op_type1)\n",
      "* (op_type2)\n",
      "( (lparen)\n",
      ") (rparen)\n",
      "13 (numeral)\n",
      "abc (identifier)\n"
     ]
    }
   ],
   "source": [
    "print(Token(\"+\"))\n",
    "print(Token(\"*\"))\n",
    "print(Token(\"(\"))\n",
    "print(Token(\")\"))\n",
    "print(Token(\"13\"))\n",
    "print(Token(\"abc\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(input_text):\n",
    "  tokens = []\n",
    "  # split the input text into a list of tokens at word boundries and whitespaces\n",
    "  # then remove empty strings and strip off leading and trailing whitespaces\n",
    "  li = [s.strip() for s in re.split(r\"\\b|\\s\", input_text, re.ASCII) \n",
    "                  if s.strip()]\n",
    "  for s in li: # s is a string\n",
    "    if not s.isascii():\n",
    "      raise ValueError(f\"'{s}' is invalid (non-ASCII)\")\n",
    "    if not (set(s).issubset(\"+-*/()\") or      # operator or parenthesis\n",
    "            (s.isdecimal() and s[0]!='0') or  # numeral\n",
    "            (s.isalnum() and s[0].isalpha() and s.islower())):   \n",
    "                                              # identifier\n",
    "      raise ValueError(f\"'{s}' is invalid (non-token)\")\n",
    "    if set(s).issubset(\"+-*/()\") and len(s) > 1:\n",
    "      # split string of consecutive operators into individual characters\n",
    "      for c in s: # c is an operator character\n",
    "        tokens.append(Token(c))\n",
    "    else:\n",
    "      tokens.append(Token(s))\n",
    "  \n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTokenizer(input_text):\n",
    "  try:\n",
    "    tokens = tokenizer(input_text)\n",
    "  except ValueError as e:\n",
    "    print(f\"Tokenizer: {e}\")\n",
    "  else:\n",
    "    for t in tokens:\n",
    "      print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first (identifier)\n",
      "+ (op_type1)\n",
      "second (identifier)\n",
      "* (op_type2)\n",
      "( (lparen)\n",
      "hello (identifier)\n",
      "+ (op_type1)\n",
      "c1 (identifier)\n",
      ") (rparen)\n",
      "+ (op_type1)\n",
      "a23 (identifier)\n"
     ]
    }
   ],
   "source": [
    "testTokenizer(\"first + second* (hello + c1)+a23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: 'Hello' is invalid (non-token)\n",
      "Tokenizer: '023' is invalid (non-token)\n"
     ]
    }
   ],
   "source": [
    "testTokenizer(\"first + second* +Hello + 23+2\")\n",
    "testTokenizer(\"first + second*-hello + 023+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "  def __init__(self, token, children=None):\n",
    "    self.token = token # the node is labeled with a Token object\n",
    "    self.children = children if children else [] # list of Node objects\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.build_polish_notation()\n",
    "\n",
    "  def build_polish_notation(self):\n",
    "    ret_str = f\"{self.token.value}\"\n",
    "    if self.children:\n",
    "      ret_str += ' '\n",
    "    ret_str += ' '.join(child.build_polish_notation() \n",
    "                        for child in self.children)\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "  def __init__(self, tokens):\n",
    "    self.tokens = tokens\n",
    "    self.current_token = None\n",
    "    self.index = -1\n",
    "    self.advance()  # set self.current_token to \n",
    "                    # the first(i.e. self.index=0) element of tokens\n",
    "\n",
    "  def advance(self): # increment self.index and set self.current_token\n",
    "    self.index += 1\n",
    "    if self.index < len(self.tokens):\n",
    "      self.current_token = self.tokens[self.index]\n",
    "    else:\n",
    "      self.current_token = None\n",
    "\n",
    "  def parse(self):\n",
    "    return self.expr() # expr() corresponds to the starting symbol <expr>\n",
    "\n",
    "  def expr(self):\n",
    "    node = self.term()\n",
    "\n",
    "    while(self.current_token is not None and  \n",
    "          self.current_token.token_type in ('op_type1')):\n",
    "      # If we are at '+' in \"a + b * c - ...\" then the next token is '-'\n",
    "      # because we will consume tokens by self.advance() and self.term().\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      right_term = self.term()\n",
    "      node = Node(token, [node, right_term]) # left associative\n",
    "\n",
    "    return node\n",
    "\n",
    "  def term(self):\n",
    "    node = self.factor()\n",
    "\n",
    "    while(self.current_token is not None and \n",
    "          self.current_token.token_type in ('op_type2')):\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      right = self.factor()\n",
    "      node = Node(token, [node, right])\n",
    "\n",
    "    return node\n",
    "\n",
    "  def factor(self):\n",
    "    if(self.current_token is not None and \n",
    "       self.current_token.token_type == 'lparen'):\n",
    "      self.advance()\n",
    "      node = self.expr()\n",
    "      if(self.current_token is not None and \n",
    "         self.current_token.token_type == 'rparen'):\n",
    "        self.advance()\n",
    "      else:\n",
    "        raise SyntaxError(\"Expected ')' after expression, in factor()\")\n",
    "    else:\n",
    "      node = self.atom()\n",
    "\n",
    "    return node\n",
    "\n",
    "  def atom(self):\n",
    "    if self.current_token is not None:\n",
    "      token = self.current_token\n",
    "      if token.token_type in ('numeral', 'identifier'):\n",
    "        self.advance()\n",
    "        return Node(token)\n",
    "      else:\n",
    "        raise SyntaxError(f\"Expected numeral or identifier, in atom(): {token}\")\n",
    "    else:\n",
    "      raise SyntaxError(\"Unexpected end of input, in atom()\")\n",
    "      \n",
    "def parse_input(input_text):\n",
    "  tokens = tokenizer(input_text)\n",
    "  parser = Parser(tokens)\n",
    "  ast = parser.parse() # ast = Abstract Syntax Tree\n",
    "  return ast\n",
    "\n",
    "def testParser(input_text):\n",
    "  try:\n",
    "    tree = parse_input(input_text)\n",
    "  except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "  except SyntaxError as e:\n",
    "    print(f\"SyntaxError: {e}\")\n",
    "  else:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + a * b - c d ab\n",
      "* + / a b 102 - const * 2 var\n",
      "ValueError: 'UpperCaseVar' is invalid (non-token)\n",
      "ValueError: 'UpperCaseVar' is invalid (non-token)\n",
      "SyntaxError: Expected numeral or identifier, in atom(): + (op_type1)\n",
      "SyntaxError: Unexpected end of input, in atom()\n",
      "SyntaxError: Expected numeral or identifier, in atom(): - (op_type1)\n"
     ]
    }
   ],
   "source": [
    "testParser(\"a + b * (c - d) + ab\")\n",
    "testParser(\"(a/b + 102)*(const - 2*var)\")\n",
    "testParser(\"c - a + UpperCaseVar\")\n",
    "testParser(\"c1 - a + UpperCaseVar\")\n",
    "testParser(\"a + + b\")\n",
    "testParser(\"a + b *\")\n",
    "testParser(\"-a + b *\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
