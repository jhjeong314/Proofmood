{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arith4 is a simple grammar for arithmetic.\n",
    "We'll build a parser for this grammar.\n",
    "\n",
    "```\n",
    "<expr> ::= <expr> (\"+\" | \"-\") <term> | <term>\n",
    "<term> ::= <term> (\"*\" | \"/\") <factor> | <factor>\n",
    "<factor> ::= \"(\" <expr> \")\" | <atom>\n",
    "<atom> ::= <identifier> | <numeral>\n",
    "<identifier> ::= <letter> { <letter>  }\n",
    "<letter> ::= [a-z] \n",
    "<numeral> ::= [1-9] { [0-9] }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "  def __init__(self, value):\n",
    "    self.value = value\n",
    "    # input value is guaranteed to be a valid token\n",
    "    if value in (\"+\", \"-\"):\n",
    "      self.token_type = 'op_type1' # precedence 1\n",
    "    elif value in (\"*\", \"/\"):\n",
    "      self.token_type = 'op_type2' # precedence 2\n",
    "    elif value == \"(\":\n",
    "      self.token_type = 'lparen'\n",
    "    elif value == \")\":\n",
    "      self.token_type = 'rparen'\n",
    "    elif value.isdecimal():\n",
    "      self.token_type = 'numeral'\n",
    "    elif value.isalpha():\n",
    "      self.token_type = 'identifier'\n",
    "    else:\n",
    "      raise ValueError(f\"'{value}' is invalid token\")\n",
    "  \n",
    "  def __str__(self):\n",
    "    return f'{self.value} ({self.token_type})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ (op_type1)\n",
      "* (op_type2)\n",
      "( (lparen)\n",
      ") (rparen)\n",
      "13 (numeral)\n",
      "abc (identifier)\n"
     ]
    }
   ],
   "source": [
    "print(Token(\"+\"))\n",
    "print(Token(\"*\"))\n",
    "print(Token(\"(\"))\n",
    "print(Token(\")\"))\n",
    "print(Token(\"13\"))\n",
    "print(Token(\"abc\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(input_text):\n",
    "  tokens = []\n",
    "  # split the input text into a list of tokens at word boundries and whitespaces\n",
    "  # then remove empty strings and strip off leading and trailing whitespaces\n",
    "  li = [s.strip() for s in re.split(r\"\\b|\\s\", input_text, re.ASCII) \n",
    "                  if s.strip()]\n",
    "  for s in li: # s is a string\n",
    "    if not s.isascii():\n",
    "      raise ValueError(f\"'{s}' is invalid (non-ASCII)\")\n",
    "    if not (set(s).issubset(\"+-*/()\") or      # operator or parenthesis\n",
    "            (s.isdecimal() and s[0]!='0') or  # numeral\n",
    "            (s.isalpha() and s.islower())):   # identifier\n",
    "      raise ValueError(f\"'{s}' is invalid (non-token)\")\n",
    "    if set(s).issubset(\"+-*/()\") and len(s) > 1:\n",
    "      # split string of consecutive operators into individual characters\n",
    "      for c in s: # c is an operator character\n",
    "        tokens.append(Token(c))\n",
    "    else:\n",
    "      tokens.append(Token(s))\n",
    "  \n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTokenizer(input_text):\n",
    "  try:\n",
    "    tokens = tokenizer(input_text)\n",
    "  except ValueError as e:\n",
    "    print(f\"Tokenizer: {e}\")\n",
    "  else:\n",
    "    for t in tokens:\n",
    "      print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first (identifier)\n",
      "+ (op_type1)\n",
      "second (identifier)\n",
      "* (op_type2)\n",
      "( (lparen)\n",
      "hello (identifier)\n",
      "+ (op_type1)\n",
      "23 (numeral)\n",
      ") (rparen)\n",
      "+ (op_type1)\n",
      "2 (numeral)\n"
     ]
    }
   ],
   "source": [
    "testTokenizer(\"first + second* (hello + 23)+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: 'Hello' is invalid (non-token)\n",
      "Tokenizer: 'c2' is invalid (non-token)\n",
      "Tokenizer: '023' is invalid (non-token)\n"
     ]
    }
   ],
   "source": [
    "testTokenizer(\"first + second* +Hello + 23+2\")\n",
    "testTokenizer(\"first + second* +hello + 23+c2\")\n",
    "testTokenizer(\"first + second*-hello + 023+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, token, children=None):\n",
    "        self.token = token\n",
    "        self.children = children if children else []\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.build_polish_notation()\n",
    "\n",
    "    def build_polish_notation(self):\n",
    "        if not self.children:\n",
    "            return str(self.token.value)\n",
    "        \n",
    "        notation = f\"{self.token.value} \"\n",
    "        notation += ' '.join(child.build_polish_notation() for child in self.children)\n",
    "        return notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.current_token = None\n",
    "        self.index = -1\n",
    "        self.advance()\n",
    "\n",
    "    def advance(self):\n",
    "        self.index += 1\n",
    "        if self.index < len(self.tokens):\n",
    "            self.current_token = self.tokens[self.index]\n",
    "        else:\n",
    "            self.current_token = None\n",
    "\n",
    "    def parse(self):\n",
    "        return self.expr()\n",
    "\n",
    "    def expr(self):\n",
    "        node = self.term()\n",
    "\n",
    "        while self.current_token is not None and self.current_token.token_type in ('op_type1'):\n",
    "            token = self.current_token\n",
    "            self.advance()\n",
    "\n",
    "            right = self.term()\n",
    "            node = Node(token, [node, right])\n",
    "\n",
    "        return node\n",
    "\n",
    "    def term(self):\n",
    "        node = self.factor()\n",
    "\n",
    "        while self.current_token is not None and self.current_token.token_type in ('op_type2'):\n",
    "            token = self.current_token\n",
    "            self.advance()\n",
    "\n",
    "            right = self.factor()\n",
    "            node = Node(token, [node, right])\n",
    "\n",
    "        return node\n",
    "\n",
    "    def factor(self):\n",
    "        if self.current_token is not None and self.current_token.token_type == 'lparen':\n",
    "            self.advance()\n",
    "            node = self.expr()\n",
    "            if self.current_token is not None and self.current_token.token_type == 'rparen':\n",
    "                self.advance()\n",
    "            else:\n",
    "                raise SyntaxError(\"Expected ')' after expression.\")\n",
    "        else:\n",
    "            node = Node(self.current_token)\n",
    "            self.advance()\n",
    "\n",
    "        return node\n",
    "\n",
    "def parse_input(input_text):\n",
    "    tokens = tokenizer(input_text)\n",
    "    parser = Parser(tokens)\n",
    "    ast = parser.parse()\n",
    "    return ast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- + a * b c 10\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"a + b * c - 10\"\n",
    "ast = parse_input(input_text)\n",
    "print(ast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ a * b - c d\n"
     ]
    }
   ],
   "source": [
    "input_text = \"a + b * (c - d)\"\n",
    "ast = parse_input(input_text)\n",
    "print(ast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
