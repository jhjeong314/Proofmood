{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arith6 is an extension of Arith5.  \n",
    "\n",
    "We add function symbols."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```perl\n",
    "<expr> ::= (<term> | <nterm>) { (\"+\" | \"-\") <term> }\n",
    "<nterm> ::= \"-\" { \"-\" } <term>\n",
    "<term> ::= <factor> { (\"*\" | \"/\") <factor> }\n",
    "<factor> ::= { <factor_exp> \"^\" } <factor_exp>\n",
    "<factor_exp> ::= <factor_post> { (\"!\" | \"'\") }\n",
    "<factor_post> ::= \"(\" <expr> \")\"  | <func_call> | <atom>\n",
    "<func_call> ::= <func_symb> '(' <expr> {',' <expr>} ')' \n",
    "  # 0-ary functions are not allowed\n",
    "<atom> ::= <identifier> | <numeral>\n",
    "<identifier> ::= <letter> { <letter> | <digit> }\n",
    "<letter> ::= [a-zA-Z] \n",
    "<numeral> ::= <nonzero_digit> { <digit> }\n",
    "<digit> ::= [0-9]\n",
    "<nonzero_digit> :: = [1-9] \n",
    "<func_symb> ::= \"sin\" | \"cos\" | \"max\" | \"min\" | \"f\"\n",
    "  # arities are given in Token constructor\n",
    "  # sin, cos are unary, max, min are binary, f are ternary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "  def __init__(self, value):\n",
    "    self.value = value\n",
    "    self.arity = None\n",
    "    # input value is guaranteed to be a valid token\n",
    "    if value == \",\":\n",
    "      self.token_type = 'comma'\n",
    "    elif value in (\"+\", \"-\"):\n",
    "      self.token_type = 'op_bin_1' # precedence 1\n",
    "    elif value in (\"*\", \"/\"):\n",
    "      self.token_type = 'op_bin_2' # precedence 2\n",
    "    elif value == \"(\":\n",
    "      self.token_type = 'lparen'\n",
    "    elif value == \")\":\n",
    "      self.token_type = 'rparen'\n",
    "    elif value in (\"!\", \"'\"):\n",
    "      self.token_type = 'op_postfix'\n",
    "    elif value == \"^\":\n",
    "      self.token_type = \"op_bin_exp\"\n",
    "    elif value.isdecimal():\n",
    "      self.token_type = 'numeral'\n",
    "    elif(value in (\"sin\", \"cos\", \"max\", \"min\", \"f\")):\n",
    "        self.token_type = 'func_symb'\n",
    "        if(value in (\"sin\", \"cos\")):\n",
    "          self.arity = 1\n",
    "        elif(value in (\"max\", \"min\")):\n",
    "          self.arity = 2\n",
    "        else:\n",
    "          self.arity = 3\n",
    "    elif value.isalnum() and value[0].isalpha():\n",
    "      self.token_type = 'identifier'\n",
    "    else:\n",
    "      raise ValueError(f\"'{value}' is invalid (Token)\")\n",
    "  \n",
    "  def __str__(self):\n",
    "    ret_str = f'{self.value} ({self.token_type})'\n",
    "    if self.arity is not None:\n",
    "      ret_str += f' arity: {self.arity}'\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", (comma)\n",
      "+ (op_bin_1), * (op_bin_2)\n",
      "( (lparen), ) (rparen)\n",
      "13 (numeral)\n",
      "abc (identifier), a1 (identifier)\n",
      "! (op_postfix), ' (op_postfix)\n",
      "^ (op_bin_exp)\n",
      "sin (func_symb) arity: 1, max (func_symb) arity: 2, f (func_symb) arity: 3\n"
     ]
    }
   ],
   "source": [
    "print(Token(\",\"))\n",
    "print(Token(\"+\"), Token(\"*\"), sep=\", \")\n",
    "print(Token(\"(\"), Token(\")\"), sep=\", \")\n",
    "print(Token(\"13\"))\n",
    "print(Token(\"abc\"), Token(\"a1\"), sep=\", \")\n",
    "print(Token(\"!\"), Token(\"'\"), sep=\", \")\n",
    "print(Token(\"^\"))\n",
    "print(Token(\"sin\"), Token(\"max\"), Token(\"f\"), sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(input_text):\n",
    "  tokens = []\n",
    "  # split the input text into a list of tokens at word boundries and whitespaces\n",
    "  # then remove empty strings and strip off leading and trailing whitespaces\n",
    "  li = [s.strip() for s in re.split(r\"\\b|\\s\", input_text, re.ASCII) \n",
    "                  if s.strip()]\n",
    "  for s in li: # s is a string\n",
    "    if not s.isascii():\n",
    "      raise ValueError(f\"'{s}' is invalid (non-ASCII)\")\n",
    "    if not (set(s).issubset(\"+-*/()!'^,\") or  # operator or parenthesis or comma\n",
    "            (s.isdecimal() and s[0]!='0') or  # numeral\n",
    "            (s.isalnum() and s[0].isalpha())):   \n",
    "                                              # identifier or function symbol\n",
    "      raise ValueError(f\"'{s}' is invalid (non-token)\")\n",
    "    if set(s).issubset(\"+-*/()!'^,\") and len(s) > 1:\n",
    "      # split string of consecutive operators into individual characters\n",
    "      for c in s: # c is an operator character\n",
    "        tokens.append(Token(c))\n",
    "    else:\n",
    "      tokens.append(Token(s))\n",
    "  \n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTokenizer(input_text):\n",
    "  try:\n",
    "    tokens = tokenizer(input_text)\n",
    "  except ValueError as e:\n",
    "    print(f\"Tokenizer: {e}\")\n",
    "  else:\n",
    "    for t in tokens:\n",
    "      print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max (func_symb) arity: 2\n",
      "( (lparen)\n",
      "a (identifier)\n",
      "/ (op_bin_2)\n",
      "b (identifier)\n",
      "! (op_postfix)\n",
      "+ (op_bin_1)\n",
      "2 (numeral)\n",
      ", (comma)\n",
      "10 (numeral)\n",
      ") (rparen)\n",
      "+ (op_bin_1)\n",
      "b1 (identifier)\n",
      "^ (op_bin_exp)\n",
      "b (identifier)\n",
      "' (op_postfix)\n",
      "* (op_bin_2)\n",
      "( (lparen)\n",
      "- (op_bin_1)\n",
      "c1 (identifier)\n",
      "! (op_postfix)\n",
      ") (rparen)\n",
      "* (op_bin_2)\n",
      "( (lparen)\n",
      "hello (identifier)\n",
      "- (op_bin_1)\n",
      "cc1 (identifier)\n",
      ") (rparen)\n",
      "+ (op_bin_1)\n",
      "a23 (identifier)\n"
     ]
    }
   ],
   "source": [
    "testTokenizer(\"max(a/b!+ 2, 10) + b1^b'*(-c1!)* (hello - cc1)+a23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: '1hello' is invalid (non-token)\n",
      "Tokenizer: '023' is invalid (non-token)\n"
     ]
    }
   ],
   "source": [
    "# some invalid inputs\n",
    "testTokenizer(\"First + Second2* +1hello + 23+2\")\n",
    "testTokenizer(\"first + second*-hello + 023+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "  def __init__(self, token, children=None):\n",
    "    self.token = token # the node is labeled with a Token object\n",
    "    self.children = children if children else [] # list of Node objects\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.build_polish_notation()\n",
    "\n",
    "  def build_polish_notation(self, opt=False):\n",
    "    ret_str = (f\"{self.token.value}({self.token.token_type})\" if opt \n",
    "      else f\"{self.token.value}\")\n",
    "    if self.children:\n",
    "      ret_str += ' '\n",
    "    ret_str += ' '.join(child.build_polish_notation(opt) \n",
    "                        for child in self.children)\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 in [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "  def __init__(self, tokens):\n",
    "    self.tokens = tokens\n",
    "    self.current_token = None\n",
    "    self.index = -1\n",
    "    self.advance()  # set self.current_token to \n",
    "                    # the first(i.e. self.index=0) element of tokens\n",
    "\n",
    "  def advance(self): # increment self.index and set self.current_token\n",
    "    self.index += 1\n",
    "    if self.index < len(self.tokens):\n",
    "      self.current_token = self.tokens[self.index]\n",
    "    else:\n",
    "      self.current_token = None\n",
    "\n",
    "  def check_token_type(self, token_types):\n",
    "    # token_types can be a string or a tuple of strings\n",
    "    # Check if self.current_token is of type token_types if token_types is a string\n",
    "    # or belongs to token_types if token_types is a tuple of strings.\n",
    "    token = self.current_token\n",
    "    if token is None:\n",
    "      return False\n",
    "    elif type(token_types) is not tuple: # must be a string in this case\n",
    "        return token.token_type == token_types\n",
    "    elif token.token_type in token_types:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "    \n",
    "  def check_token_value(self, token_value):\n",
    "    # Check if self.current_token is of value token_value.\n",
    "    token = self.current_token\n",
    "    if token is None:\n",
    "      return False\n",
    "    elif token.value == token_value:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "    \n",
    "  def parse(self):\n",
    "    return self.expr() # expr() corresponds to the starting symbol <expr>\n",
    "\n",
    "  def expr(self):\n",
    "    if self.check_token_value('-'): # unary minus\n",
    "      node = self.nterm() # negative term\n",
    "    else: # ordinary term\n",
    "      node = self.term()  \n",
    "\n",
    "    while self.check_token_type('op_bin_1'): # '+' or '-'\n",
    "      # If we are at '+' in \"a + b * c - ...\" then the next token is '-'\n",
    "      # because we will consume tokens by self.advance() and self.term().\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      right_term = self.term()\n",
    "      node = Node(token, [node, right_term]) # left associative\n",
    "    \n",
    "    return node\n",
    "  \n",
    "  def nterm(self):\n",
    "    token = self.current_token \n",
    "    # For the first visit only, token.value == '-' is  guaranteed\n",
    "    #   because we have checked it in self.expr().\n",
    "    # But for subsequent recursive calls it can be otherwise.\n",
    "    if(token is None or token.value != '-'):\n",
    "      node = self.term()\n",
    "    else:\n",
    "      token.token_type = 'op_unary_prefix'\n",
    "      self.advance()\n",
    "      unary_node = self.nterm() # recursive call\n",
    "      node = Node(token, [unary_node])\n",
    "\n",
    "    return node\n",
    "  \n",
    "  def term(self):\n",
    "    node = self.factor()\n",
    "\n",
    "    while self.check_token_type('op_bin_2'): # '*' or '/'\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      right_factor = self.factor()\n",
    "      node = Node(token, [node, right_factor])\n",
    "\n",
    "    return node\n",
    "\n",
    "  def factor(self):\n",
    "    node = self.factor_exp()\n",
    "\n",
    "    if self.check_token_type('op_bin_exp'): # '^'\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      right_factor = self.factor() # recursive call for right associativity\n",
    "      node = Node(token, [node, right_factor])\n",
    "\n",
    "    return node\n",
    "  \n",
    "  def factor_exp(self):\n",
    "    node = self.factor_postfix()\n",
    "\n",
    "    while self.check_token_type('op_postfix'):\n",
    "      token = self.current_token\n",
    "      self.advance()\n",
    "      node = Node(token, [node])\n",
    "\n",
    "    return node\n",
    "\n",
    "  def factor_postfix(self):\n",
    "    if self.check_token_type('lparen'):\n",
    "      self.advance()\n",
    "      node = self.expr()\n",
    "      if self.check_token_type('rparen'):\n",
    "        self.advance()\n",
    "      else:\n",
    "        raise SyntaxError(\"Expected ')' after expression at {self.index} in factor_postfix(), but {self.current_token} is given.\")\n",
    "    elif self.check_token_type('func_symb'):\n",
    "      node = self.func_call()\n",
    "    else:\n",
    "      node = self.atom()\n",
    "\n",
    "    return node\n",
    "\n",
    "  def func_call(self):\n",
    "    if self.current_token is not None:\n",
    "      token = self.current_token\n",
    "      if self.check_token_type('func_symb'):\n",
    "        self.advance()\n",
    "        if self.check_token_type('lparen'):\n",
    "          self.advance()\n",
    "          args = []\n",
    "\n",
    "          while True:\n",
    "            args.append(self.expr())\n",
    "            if self.check_token_type('comma'):\n",
    "              self.advance()\n",
    "            elif self.check_token_type('rparen'):\n",
    "              break\n",
    "            else:\n",
    "              raise SyntaxError(f\"Expected ',' or ')' after function argument at {self.index} in func_call(), but {self.current_token} is given.\")\n",
    "          \n",
    "          # arity check\n",
    "          if token.arity is None or token.arity != len(args):\n",
    "            raise SyntaxError(f\"Function {token.value} expects {token.arity} arguments, but {len(args)} were given\")\n",
    "\n",
    "          self.advance()\n",
    "          return Node(token, args)\n",
    "        \n",
    "        else:\n",
    "          raise SyntaxError(f\"Expected '(' after function symbol at {self.index} in func_call(), but {self.current_token} is given.\")\n",
    "      else:\n",
    "        raise SyntaxError(f\"Expected function symbol at {self.index} in func_call(), but {token} is given.\")\n",
    "    else:\n",
    "      raise SyntaxError(\"Unexpected end of input, in func_call()\")\n",
    "      \n",
    "  def atom(self):\n",
    "    if self.current_token is not None:\n",
    "      token = self.current_token\n",
    "      if self.check_token_type(('numeral', 'identifier')):\n",
    "        self.advance()\n",
    "        return Node(token)\n",
    "      else:\n",
    "        raise SyntaxError(f\"Expected numeral or identifier at {self.index}, in atom(), but {token} is given.\")\n",
    "    else:\n",
    "      raise SyntaxError(\"Unexpected end of input, in atom()\")\n",
    "      \n",
    "def parse_input(input_text):\n",
    "  tokens = tokenizer(input_text)\n",
    "  parser = Parser(tokens)\n",
    "  ast = parser.parse() # ast = Abstract Syntax Tree\n",
    "  if parser.current_token is not None:\n",
    "    raise SyntaxError(f\"Unexpected token {parser.current_token} at {parser.index}, in parse_input(). Expected end of input.\")\n",
    "  return ast\n",
    "\n",
    "def testParser(input_text, showOperType=False):\n",
    "  try:\n",
    "    ast = parse_input(input_text)\n",
    "  except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "  except SyntaxError as e:\n",
    "    print(f\"SyntaxError: {e}\")\n",
    "  else:\n",
    "    print(ast.build_polish_notation(showOperType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ a f sin a max b + - / a ! ' b 3 12\n"
     ]
    }
   ],
   "source": [
    "testParser(\"a + f(sin(a),max(b, -a/b'! + 3), 12)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntaxError: Function f expects 3 arguments, but 2 were given\n",
      "SyntaxError: Expected numeral or identifier at 8, in atom(), but ) (rparen) is given.\n"
     ]
    }
   ],
   "source": [
    "# Some illegal expressions\n",
    "testParser(\"a + f(a, b)\")\n",
    "testParser(\"a + f(a, b, )\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
